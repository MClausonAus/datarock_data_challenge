[
  {
    "objectID": "scripts/datarock_code_challenge.html",
    "href": "scripts/datarock_code_challenge.html",
    "title": "Datarock Data Challenge",
    "section": "",
    "text": "Investigate data and screen models for classification.\n\n\nCode\nlibrary(tidyverse)\nlibrary(here)\nlibrary(visdat)\nlibrary(dlookr)\nlibrary(robCompositions)\nlibrary(brms)\nlibrary(cmdstanr)\nlibrary(rsample)\n\n## ggplot theme\ntheme_mc &lt;- function(base_size = 20) {\n    theme_bw(base_size = base_size) %+replace%\n        theme(\n            # margins\n            plot.title = element_text(size = rel(1), face = \"bold\", margin = margin(0, 0, 5, 0), hjust = 0),\n            # grids\n            panel.grid.minor = element_blank(),\n            panel.border = element_blank(),\n            # axes\n            axis.title = element_text(size = rel(0.75), face = \"bold\"),\n            axis.text = element_text(size = rel(0.60), face = \"bold\"),\n            axis.line = element_line(color = \"black\", arrow = arrow(length = unit(0.3, \"lines\"), type = \"closed\")),\n            # Legend\n            legend.title = element_text(size = rel(0.75), face = \"bold\"),\n            legend.text = element_text(size = rel(0.60), face = \"bold\"),\n            legend.key = element_rect(fill = \"transparent\", colour = NA),\n            legend.key.size = unit(1.5, \"lines\"),\n            legend.background = element_rect(fill = \"transparent\", colour = NA),\n            legend.position = \"bottom\",\n            # facets\n            strip.background = element_rect(fill = \"#17252D\", color = \"#17252D\"),\n            strip.text = element_text(size = rel(0.75), face = \"bold\", color = \"white\", margin = margin(5, 0, 5, 0))\n        )\n}\n\ntheme_set(theme_mc())\n\n# functions\n# deal with \"&lt;\" in Au\nconvert_and_replace &lt;- function(x) {\n  # find rows that have \"&lt;\"\n  if_else(grepl(\"&lt;\", x), \n          as.numeric(sub(\"&lt;\", \"\", x)) / 2,  # strip character, divide by two\n          as.numeric(x))                     # else convert to numeric\n}\n\n# CoDA PCA\n\nperform_codapca &lt;- function(data, additional_data = NULL, pca_method = \"robust\") {\n  # Perform PCA\n  pca_result &lt;- pcaCoDa(data, method = pca_method)\n  \n  summary_result &lt;- summary(pca_result)\n  \n  # Extract scores from PCA\n  scores &lt;- as.data.frame(pca_result$scores)\n  scores$id &lt;- rownames(scores)\n  \n  # If there is additional data to bind, bind it\n  if (!is.null(additional_data)) {\n    scores &lt;- scores %&gt;% bind_cols(additional_data)\n  }\n  \n  # Extract loadings\n  loadings &lt;- as.data.frame(pca_result$loadings)\n  loading_labels &lt;- rownames(loadings)\n  loadings &lt;- cbind(Feature = loading_labels, loadings)\n  \n  # Return scores and loadings as separate data frames\n  return(list(summary_result, scores = scores, loadings = loadings))\n}\n\n# plot biplot \n\nplot_biplot_var &lt;- function(score_df, loading_df, colour_var, pc_x, pc_y, scaling_var = 4){\n  \n  # Ensure the PC columns exist in the data frames\n  pc_x_col &lt;- paste0(\"PC\", pc_x)\n  pc_y_col &lt;- paste0(\"PC\", pc_y)\n  \n  if(!(pc_x_col %in% names(score_df)) | !(pc_y_col %in% names(score_df))){\n    stop(\"Specified principal components are not found in the scores data frame.\")\n  }\n  \n  if(!(pc_x_col %in% names(loading_df)) | !(pc_y_col %in% names(loading_df))){\n    stop(\"Specified principal components are not found in the loadings data frame.\")\n  }\n  \n  # Combine score_df with the selected colour variable\n  combined_df &lt;- score_df %&gt;%\n    bind_cols(colour_var = class_df[[colour_var]])\n  \n  # Create the biplot\n  ggplot(combined_df, aes_string(x = pc_x_col, y = pc_y_col)) +\n    geom_point(aes_string(colour = \"colour_var\", alpha = 0.4)) +\n    #scale_color_viridis_c() +  # Uncomment if you want to use viridis color scale\n    geom_segment(\n      data = loading_df,\n      aes_string(\n        x = 0,\n        y = 0,\n        xend = paste0(pc_x_col, \" * \", scaling_var),\n        yend = paste0(pc_y_col, \" * \", scaling_var)\n      ),\n      arrow = arrow(\n        length = unit(0.3, \"cm\"),\n        type = \"open\",\n        angle = 25\n      ),\n      size = 1,\n      color = \"darkblue\"\n    ) +\n    ggrepel::geom_text_repel(data = loading_df, aes_string(x = paste0(pc_x_col, \" * \", scaling_var), y = paste0(pc_y_col, \" * \", scaling_var), label = \"row.names(loading_df)\")) +\n    labs(title = \"Log Ratio PCA | All\", x = pc_x_col, y = pc_y_col)\n}\n\nset_cmdstan_path(\"C:\\\\Users\\\\mclauson\\\\cmdstan-2.34.1\")\nmulti_metric &lt;- yardstick::metric_set(yardstick::rsq, yardstick::mae, yardstick::rmse)\n\n\n\n\nCode\ndata_for_distribution &lt;- read_csv(here(\"data/data_for_distribution.csv\"))"
  },
  {
    "objectID": "scripts/datarock_code_challenge.html#coda-pca",
    "href": "scripts/datarock_code_challenge.html#coda-pca",
    "title": "Datarock Data Challenge",
    "section": "CoDA PCA",
    "text": "CoDA PCA\n\nPlot shows strong zonation in the first two principal components.\nPb, Au, Mo strongly correlated more associated with Class A.\nCu, Fe, Zn are positively correlated, weaker positively correlated to As & negatively correlated to Pb, Au, Mo. Associated with Class B.\nS orthogonal to other elements, indicating no correlation also appears to have little relationship to Class.\n\n\n\nCode\nout.pca &lt;- ToolsForCoDa::lrpca(n_df)\nFp &lt;- out.pca$Fp\nGs &lt;- out.pca$Gs\ndecom &lt;- out.pca$decom\n\nscores_df &lt;- as.data.frame(Fp)\nloadings_df &lt;- as.data.frame(Gs)\nnames(loadings_df) &lt;- paste0(\"PC\", 1:ncol(loadings_df))\n\nplot_biplot_var(scores_df, loadings_df, \"Class\", 1,2, 4)\n\n\n\n\n\n\n\n\n\nRather than go down the ILR and single binary partition plots, going to use log-ratios of the elements."
  },
  {
    "objectID": "scripts/datarock_code_challenge.html#log-ratio-analysis",
    "href": "scripts/datarock_code_challenge.html#log-ratio-analysis",
    "title": "Datarock Data Challenge",
    "section": "Log Ratio Analysis",
    "text": "Log Ratio Analysis\nThe element As is only in one of the log-ratios, might be able to remove this column which would allow more data to be kept for training the model without affecting the overall model too much.\nOtherwise curiously the Fe appears to be important.\n\n\nCode\nlr_res &lt;- easyCODA::STEP(n_df)\n\nex_var &lt;- lr_res$R2max\nlr_selected &lt;- lr_res$names\n\n# print lr\nprint(lr_selected)\n\n\n[1] \"Fe/S\"  \"Pb/Fe\" \"Fe/Zn\" \"As/Mo\" \"Fe/Mo\" \"Fe/Cu\" \"Au/Fe\"\n\n\nCode\n# plot\nvar1 &lt;- bind_cols(ex_var, lr_selected) %&gt;% \n  rename(ex_var = ...1, log_ratio = ...2)\n\nggplot(var1) +\n  aes(x = ex_var, y = reorder(log_ratio, ex_var)) +\n  geom_bar(stat = \"summary\", fun = \"sum\", fill = \"#112446\") +\n  labs(title = \"Variance Explained by Log Ratios\")"
  },
  {
    "objectID": "scripts/datarock_code_challenge.html#logistic",
    "href": "scripts/datarock_code_challenge.html#logistic",
    "title": "Datarock Data Challenge",
    "section": "Logistic",
    "text": "Logistic\nTrial log-ratio approach as data is compositional.\nUse horseshoe prior to select variables.\n\n\nCode\nreg_df &lt;- bind_cols(n_df, class_df)\n\nlr_reg_df &lt;- reg_df %&gt;% \n  mutate(fe_s = Fe/S, pb_fe = Pb/Fe, fe_zn = Fe/Zn, as_mo = As/Mo, fe_mo = Fe/Mo, fe_cu = Fe/Cu, au_fe = Au/Fe) %&gt;% \n  select(fe_s:au_fe, Class)\n\n# split data \n\nset.seed(123) \nsplit &lt;- initial_split(lr_reg_df, prop = 0.8)\n\n# Extract training and testing sets\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\n\n\n\nCode\nmod1_hs &lt;- brm(\n  Class ~ .,\n  data = train_data,\n  family = bernoulli(),\n  prior = set_prior(horseshoe(1)),\n  file_refit = \"on_change\",\n  file = here(\"fits\", \"mod1_hs\"),\n  backend = \"cmdstanr\",\n  iter = 2000,\n  cores = 16,\n  threads = threading(4),\n  control = list(adapt_delta = 0.9, max_treedepth = 12)\n)\n\nprint(mod1_hs)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Class ~ fe_s + pb_fe + fe_zn + as_mo + fe_mo + fe_cu + au_fe \n   Data: train_data (Number of observations: 2001) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.36      0.17    -0.70    -0.04 1.01     1556     2910\nfe_s          0.00      0.00    -0.00     0.00 1.05       52     1342\npb_fe       -41.07      6.49   -55.10   -28.75 1.03     2074     2574\nfe_zn        -0.00      0.00    -0.00    -0.00 1.00     1093     1982\nas_mo         0.06      0.01     0.05     0.08 1.02     2179     3158\nfe_mo         0.00      0.00     0.00     0.00 1.00     2152     3119\nfe_cu         0.00      0.00     0.00     0.00 1.01      458      968\nau_fe        -0.01      0.31    -0.32     0.14 1.03      180      387\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\n# PP Check\npp_check(mod1_hs)\n\n\n\n\n\n\n\n\n\nRefit model with reduced variables.\n\n\nCode\n# refit model\nmod2_simple &lt;- brm(\n  Class ~ pb_fe + as_mo + au_fe,\n  data = train_data,\n  family = bernoulli(),\n  file_refit = \"on_change\",\n  file = here(\"fits\", \"mod2_simple\"),\n  backend = \"cmdstanr\",\n  iter = 2000,\n  cores = 16,\n  threads = threading(4),\n  control = list(adapt_delta = 0.9, max_treedepth = 12)\n)\n\nprint(mod2_simple)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: Class ~ pb_fe + as_mo + au_fe \n   Data: train_data (Number of observations: 2001) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n            Estimate Est.Error   l-95% CI   u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept      -0.78      0.11      -0.99      -0.57 1.00     3662     3339\npb_fe         -43.04      7.11     -56.69     -29.43 1.00     2309     2578\nas_mo           0.08      0.01       0.07       0.10 1.00     3051     3066\nau_fe     -357862.57  77424.20 -518622.77 -215236.23 1.00     2393     2366\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\npp_check(mod2_simple, type = \"bars\")\n\n\n\n\n\n\n\n\n\nCode\npredicted_probs &lt;- broom.mixed::augment(mod2_simple, newdata = test_data)\n\naugmented_test_data &lt;- predicted_probs %&gt;%\n  mutate(\n    predicted_class = ifelse(.fitted &gt; 0.5, 1, 0),\n    Class_num = if_else(Class == \"A\", 0, 1),\n    f_pred = as.factor(predicted_class),\n    f_actual = as.factor(Class_num)\n  )\n\n# Step 2: Confusion matrix and F1 score calculation\n# Confusion Matrix\nconf_mat &lt;- yardstick::conf_mat(augmented_test_data, truth = f_actual, estimate = f_pred)\n\n# F1 Score\nf1 &lt;- yardstick::f_meas(augmented_test_data, truth = f_actual, estimate = f_pred)\n\n# Output the confusion matrix and F1 score\nprint(conf_mat)\n\n\n          Truth\nPrediction   0   1\n         0 336  66\n         1  28  71\n\n\nCode\nprint(f1)\n\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  binary         0.877\n\n\nf score is okay, looks like might be suffering from class imbalance which might be helped with SMOTE etc."
  },
  {
    "objectID": "scripts/datarock_code_challenge.html#black-box",
    "href": "scripts/datarock_code_challenge.html#black-box",
    "title": "Datarock Data Challenge",
    "section": "Black Box",
    "text": "Black Box\nScreen many models to assess if classification is viable.\n\n\nCode\nlibrary(tidymodels)\nlibrary(bestNormalize)\nlibrary(baguette)\nlibrary(discrim)\nlibrary(embed)\nlibrary(themis)\n\nlr_reg_df &lt;- reg_df %&gt;% \n  mutate(fe_s = Fe/S, pb_fe = Pb/Fe, fe_zn = Fe/Zn, as_mo = As/Mo, fe_mo = Fe/Mo, fe_cu = Fe/Cu, au_fe = Au/Fe) %&gt;% \n  mutate(Class = as.factor(Class))\n\n# split data \n\nset.seed(123) \nsplit &lt;- initial_validation_split(lr_reg_df, prop = c(0.75, 0.125), strata = Class)\n\n# Extract training and testing sets\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\nvalid_data &lt;- vfold_cv(lr_reg_df)\n\n\n\n\nCode\nrec &lt;- recipe(Class ~ ., data = train_data) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors()) %&gt;% \n  # try using SMOTE\n  step_smote(Class)\n\nmlp_spec &lt;-\n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;%\n  set_engine('nnet') %&gt;%\n  set_mode('classification')\n\nbagging_spec &lt;-\n  bag_tree() %&gt;%\n  set_engine('rpart') %&gt;%\n  set_mode('classification')\n\nfda_spec &lt;-\n  discrim_flexible(\n    prod_degree = tune()\n  ) %&gt;%\n  set_engine('earth')\n\nrda_spec &lt;-\n  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %&gt;%\n  set_engine('klaR')\n\nbayes_spec &lt;-\n  naive_Bayes() %&gt;%\n  set_engine('klaR')\n\npls_rec &lt;- \n  rec %&gt;% \n  step_pls(all_numeric_predictors(), outcome = \"Class\", num_comp = tune())\n\npca_rec &lt;- \n  rec %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = tune()) \n\n\n\n\nCode\nctrl &lt;- control_grid(parallel_over = \"everything\")\n\nres &lt;- \n  workflow_set(\n    preproc = list(basic = Class ~., pls = pls_rec, pca = pca_rec),  \n    models = list(bayes = bayes_spec, fda = fda_spec,\n                  rda = rda_spec, bag = bagging_spec,\n                  mlp = mlp_spec)\n  ) %&gt;% \n  workflow_map(\n    verbose = TRUE,\n    seed = 199,\n    resamples = valid_data,\n    grid = 10,\n    metrics = metric_set(f_meas),\n    control = ctrl\n  )\n\n\n\n\nCode\nrankings &lt;- \n  rank_results(res, select_best = TRUE) %&gt;% \n  mutate(method = map_chr(wflow_id, ~ str_split(.x, \"_\", simplify = TRUE)[1])) \n\nggplot(rankings, aes(x = rank, y = mean, color = model, shape = method)) +\n  geom_point(size = 4) +  # Add points\n  geom_text(aes(label = wflow_id), vjust = -0.5, size = 3) +  # Add labels\n  labs(x = \"rank\", y = \"F-Score\", title = \"Model Performance by F-Score\")\n\n\n\n\n\n\n\n\n\nBagged tree classifies quite well but could be overfitting? Interestingly, dimension reduction techniques don’t appear to be helpful for this problem."
  }
]