---
title: "Datarock Data Challenge"
author: "Matt Clauson"
date: "`r Sys.Date()`"  
output:
  html_document:
    code_folding: hide      
    toc: true               
    toc_float: true         
    toc_depth: 3
execute: 
  cache: refresh
---

# Introduction

Investigate data and screen models for clasification. 

```{r setup}

library(tidyverse)
library(here)
library(visdat)
library(dlookr)
library(robCompositions)
library(brms)
library(cmdstanr)
library(rsample)

## ggplot theme
theme_mc <- function(base_size = 16) {
    theme_bw(base_size = base_size) %+replace%
        theme(
            # margins
            plot.title = element_text(size = rel(1), face = "bold", margin = margin(0, 0, 5, 0), hjust = 0),
            # grids
            panel.grid.minor = element_blank(),
            panel.border = element_blank(),
            # axes
            axis.title = element_text(size = rel(0.75), face = "bold"),
            axis.text = element_text(size = rel(0.60), face = "bold"),
            axis.line = element_line(color = "black", arrow = arrow(length = unit(0.3, "lines"), type = "closed")),
            # Legend
            legend.title = element_text(size = rel(0.75), face = "bold"),
            legend.text = element_text(size = rel(0.60), face = "bold"),
            legend.key = element_rect(fill = "transparent", colour = NA),
            legend.key.size = unit(1.5, "lines"),
            legend.background = element_rect(fill = "transparent", colour = NA),
            legend.position = "bottom",
            # facets
            strip.background = element_rect(fill = "#17252D", color = "#17252D"),
            strip.text = element_text(size = rel(0.75), face = "bold", color = "white", margin = margin(5, 0, 5, 0))
        )
}

theme_set(theme_mc())

# functions
# deal with "<" in Au
convert_and_replace <- function(x) {
  # find rows that have "<"
  if_else(grepl("<", x), 
          as.numeric(sub("<", "", x)) / 2,  # strip character, divide by two
          as.numeric(x))                     # else convert to numeric
}

# CoDA PCA

perform_codapca <- function(data, additional_data = NULL, pca_method = "robust") {
  # Perform PCA
  pca_result <- pcaCoDa(data, method = pca_method)
  
  summary_result <- summary(pca_result)
  
  # Extract scores from PCA
  scores <- as.data.frame(pca_result$scores)
  scores$id <- rownames(scores)
  
  # If there is additional data to bind, bind it
  if (!is.null(additional_data)) {
    scores <- scores %>% bind_cols(additional_data)
  }
  
  # Extract loadings
  loadings <- as.data.frame(pca_result$loadings)
  loading_labels <- rownames(loadings)
  loadings <- cbind(Feature = loading_labels, loadings)
  
  # Return scores and loadings as separate data frames
  return(list(summary_result, scores = scores, loadings = loadings))
}

# plot biplot 

plot_biplot_var <- function(score_df, loading_df, colour_var, pc_x, pc_y, scaling_var = 4){
  
  # Ensure the PC columns exist in the data frames
  pc_x_col <- paste0("PC", pc_x)
  pc_y_col <- paste0("PC", pc_y)
  
  if(!(pc_x_col %in% names(score_df)) | !(pc_y_col %in% names(score_df))){
    stop("Specified principal components are not found in the scores data frame.")
  }
  
  if(!(pc_x_col %in% names(loading_df)) | !(pc_y_col %in% names(loading_df))){
    stop("Specified principal components are not found in the loadings data frame.")
  }
  
  # Combine score_df with the selected colour variable
  combined_df <- score_df %>%
    bind_cols(colour_var = class_df[[colour_var]])
  
  # Create the biplot
  ggplot(combined_df, aes_string(x = pc_x_col, y = pc_y_col)) +
    geom_point(aes_string(colour = "colour_var", alpha = 0.4)) +
    #scale_color_viridis_c() +  # Uncomment if you want to use viridis color scale
    geom_segment(
      data = loading_df,
      aes_string(
        x = 0,
        y = 0,
        xend = paste0(pc_x_col, " * ", scaling_var),
        yend = paste0(pc_y_col, " * ", scaling_var)
      ),
      arrow = arrow(
        length = unit(0.3, "cm"),
        type = "open",
        angle = 25
      ),
      size = 1,
      color = "darkblue"
    ) +
    ggrepel::geom_text_repel(data = loading_df, aes_string(x = paste0(pc_x_col, " * ", scaling_var), y = paste0(pc_y_col, " * ", scaling_var), label = "row.names(loading_df)")) +
    labs(title = "Log Ratio PCA | All", x = pc_x_col, y = pc_y_col)
}

set_cmdstan_path("C:\\Users\\mclauson\\cmdstan-2.34.1")
multi_metric <- yardstick::metric_set(yardstick::rsq, yardstick::mae, yardstick::rmse)


```

```{r}
data_for_distribution <- read_csv("~/Personal/graddip_math_stat/code_challenge/datarock/data/data_for_distribution.csv")
```

# Cleaning 

```{r}
vis_dat(data_for_distribution)
```

First thing Au is character, As has large amount missing.

```{r}
df <- data_for_distribution %>% 
  mutate(Au = as.numeric(Au))
```

Converting to numeric introduces NAs for Au, checking due to LOD (<), symbol needs to be dealt with. 

Convert Au to numeric, and check for default values. 

```{r}
df <- data_for_distribution %>% 
  mutate(Au = convert_and_replace(Au))
```

Quick check for defaults e.g. -99. Can't have negative values for assays. 

```{r} 
vis_expect(df %>% select(where(is.numeric)), ~.x >= 0) # only interested in numeric (not strictly from/to)
```

Check compositing of the drill holes

```{r}
df %>%
  mutate(composite_length = to - from) %>%
  group_by(holeid) %>%
  summarise(
    mean_length = mean(composite_length),
    min_length = min(composite_length),
    max_length = max(composite_length)
  ) %>%
  filter(mean_length != 10)
```

Check for duplicate ID 

```{r}
df %>%
  summarise(across(c(Unique_ID, holeid), 
                   ~ n_distinct(.) / n() * 100, 
                   .names = "percent_unique_{col}"))
```

Check Classes 

```{r}
df %>%
  group_by(Class) %>% 
  summarise(count = n(), 
            count_pct = n() / nrow(df) * 100)
```

Missing class label for ~16% of the rows. 

Replace default values and ? with NA and view relationship to missing classes.

```{r}
df <- df %>%
  mutate(across(everything(), ~ ifelse(. == -999 | . == "?", NA, .)))
```

Check intersection of missingness.

```{r}
naniar::gg_miss_var(df, show_pct = TRUE, facet = Class)
```

### Summary

- from/to & sampleid are complete.
- Replace LOD Au values with LOD/2 as a quick fix.
- Default values replaced with NA.
- ? Class replaced with NA to make the missingness clearer. 

# EDA

Check normality, distribution, PCA, CoDA.

Removal of ? and other NA values.

Dropping the ? Class rows as these can't be used for training the model.
Dropping NA from the assays as without more information struggling to justify imputing data or removal of column.

```{r}
df %>% 
  select(As:Zn) %>% 
  normality()
```

```{r}
df %>% 
  group_by(Class) %>% 
  select(As:Zn) %>% 
  describe() %>% 
  select(described_variables:kurtosis,p25,p50,p75,p100)
```

### Raincloud Plots

Check distribution of numeric variables.

```{r}
n_df <- df %>% select(As:Zn, Class) %>% drop_na() %>% select(-Class) %>% data.frame()
class_df <- df %>% select(As:Zn, Class) %>% drop_na() %>% select(Class)

n_df %>% 
  bind_cols(class_df) %>% 
  pivot_longer(!Class) %>% 
  #filter(name == "As") %>% 
  ggplot(aes(x = Class, y = value, fill = name)) + 
  ggdist::stat_halfeye(
    adjust = .1, 
    width = .5, 
    .width = 0, 
    justification = -.3, 
    point_colour = NA) + 
  geom_boxplot(
    width = .25, 
    outlier.shape = NA
  ) +
  geom_point(
    size = 0.1,
    alpha = .2,
    position = position_jitter(
      seed = 1, width = .05
    )
  ) + 
  coord_cartesian(clip = "off") +
  facet_wrap(~name, scales = "free")
```


## CoDA PCA

- Plot shows strong zonation in the first two principal components.
- Pb, Au, Mo strongly correlated more associated with Class A.
- Cu, Fe, Zn are positively correlated, weaker positively correlated to As & negatively correlated to Pb, Au, Mo. Associated with Class B.
- S orthogonal to other elements, indicating no correlation also appears to have little relationship to Class.


```{r}


out.pca <- ToolsForCoDa::lrpca(n_df)
Fp <- out.pca$Fp
Gs <- out.pca$Gs
decom <- out.pca$decom

scores_df <- as.data.frame(Fp)
loadings_df <- as.data.frame(Gs)
names(loadings_df) <- paste0("PC", 1:ncol(loadings_df))

plot_biplot_var(scores_df, loadings_df, "Class", 1,2, 4)
```

Rather than go down the ILR and single binary partition plots, going to use log-ratios of the elements. 

## Log Ratio Analysis

As is only in one of the log-ratios, might be able to remove this column which would allow more data to be kept for training the model without affecting the overall model too much.

Otherwise curiously the Fe appears to be important.

```{r}
lr_res <- easyCODA::STEP(n_df)

ex_var <- lr_res$R2max
lr_selected <- lr_res$names

# print lr
print(lr_selected)

# plot
var1 <- bind_cols(ex_var, lr_selected) %>% 
  rename(ex_var = ...1, log_ratio = ...2)

ggplot(var1) +
  aes(x = ex_var, y = reorder(log_ratio, ex_var)) +
  geom_bar(stat = "summary", fun = "sum", fill = "#112446") +
  labs(title = "Variance Explained by Log Ratios")
```

# Classification

## Logistic

Trial log-ratio approach as data is compositional.

Use horseshoe prior to select variables.


```{r}
reg_df <- bind_cols(n_df, class_df)

lr_reg_df <- reg_df %>% 
  mutate(fe_s = Fe/S, pb_fe = Pb/Fe, fe_zn = Fe/Zn, as_mo = As/Mo, fe_mo = Fe/Mo, fe_cu = Fe/Cu, au_fe = Au/Fe) %>% 
  select(fe_s:au_fe, Class)

# split data 

set.seed(123) 
split <- initial_split(lr_reg_df, prop = 0.8)

# Extract training and testing sets
train_data <- training(split)
test_data <- testing(split)
```

```{r}
mod1_hs <- brm(
  Class ~ .,
  data = train_data,
  family = bernoulli(),
  prior = set_prior(horseshoe(1)),
  file_refit = "on_change",
  file = here("fits", "mod1_hs"),
  backend = "cmdstanr",
  iter = 2000,
  cores = 16,
  threads = threading(4),
  control = list(adapt_delta = 0.9, max_treedepth = 12)
)

print(mod1_hs)

# PP Check
pp_check(mod1_hs)

# refit model
mod2_simple <- brm(
  Class ~ pb_fe + as_mo + au_fe,
  data = train_data,
  family = bernoulli(),
  file_refit = "on_change",
  file = here("fits", "mod2_simple"),
  backend = "cmdstanr",
  iter = 2000,
  cores = 16,
  threads = threading(4),
  control = list(adapt_delta = 0.9, max_treedepth = 12)
)

pp_check(mod2_simple, type = "bars")

predicted_probs <- broom.mixed::augment(mod2_simple, newdata = test_data)

augmented_test_data <- predicted_probs %>%
  mutate(
    predicted_class = ifelse(.fitted > 0.5, 1, 0),
    Class_num = if_else(Class == "A", 0, 1),
    f_pred = as.factor(predicted_class),
    f_actual = as.factor(Class_num)
  )

# Step 2: Confusion matrix and F1 score calculation
# Confusion Matrix
conf_mat <- yardstick::conf_mat(augmented_test_data, truth = f_actual, estimate = f_pred)

# F1 Score
f1 <- yardstick::f_meas(augmented_test_data, truth = f_actual, estimate = f_pred)

# Output the confusion matrix and F1 score
print(conf_mat)
print(f1)
```

f score is okay, looks like might be suffering from class imbalance which might be helped with SMOTE etc.

## Black Box 

Screen many models.

```{r}
library(tidymodels)
library(bestNormalize)
library(baguette)
library(discrim)
library(embed)
library(themis)

lr_reg_df <- reg_df %>% 
  mutate(fe_s = Fe/S, pb_fe = Pb/Fe, fe_zn = Fe/Zn, as_mo = As/Mo, fe_mo = Fe/Mo, fe_cu = Fe/Cu, au_fe = Au/Fe) %>% 
  mutate(Class = as.factor(Class))

# split data 

set.seed(123) 
split <- initial_validation_split(lr_reg_df, prop = c(0.75, 0.125), strata = Class)

# Extract training and testing sets
train_data <- training(split)
test_data <- testing(split)
valid_data <- vfold_cv(lr_reg_df)

```

```{r}
rec <- recipe(Class ~ ., data = train_data) %>%
  step_zv(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  # try using SMOTE
  step_smote(Class)

mlp_spec <-
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%
  set_engine('nnet') %>%
  set_mode('classification')

bagging_spec <-
  bag_tree() %>%
  set_engine('rpart') %>%
  set_mode('classification')

fda_spec <-
  discrim_flexible(
    prod_degree = tune()
  ) %>%
  set_engine('earth')

rda_spec <-
  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %>%
  set_engine('klaR')

bayes_spec <-
  naive_Bayes() %>%
  set_engine('klaR')

pls_rec <- 
  rec %>% 
  step_pls(all_numeric_predictors(), outcome = "Class", num_comp = tune())

pca_rec <- 
  rec %>%
  step_pca(all_numeric_predictors(), num_comp = tune()) 


```

```{r}
ctrl <- control_grid(parallel_over = "everything")

res <- 
  workflow_set(
    preproc = list(basic = Class ~., pls = pls_rec, pca = pca_rec),  
    models = list(bayes = bayes_spec, fda = fda_spec,
                  rda = rda_spec, bag = bagging_spec,
                  mlp = mlp_spec)
  ) %>% 
  workflow_map(
    verbose = TRUE,
    seed = 199,
    resamples = valid_data,
    grid = 10,
    metrics = metric_set(f_meas),
    control = ctrl
  )
```

```{r}
rankings <- 
  rank_results(res, select_best = TRUE) %>% 
  mutate(method = map_chr(wflow_id, ~ str_split(.x, "_", simplify = TRUE)[1])) 

ggplot(rankings, aes(x = rank, y = mean, color = model, shape = method)) +
  geom_point(size = 4) +  # Add points
  geom_text(aes(label = wflow_id), vjust = -0.5, size = 3) +  # Add labels
  labs(x = "rank", y = "F-Score", title = "Model Performance by F-Score")
```

# Conclusion

Classification of the A & B zones seems possible with varying performance achieved by different model types.
Further background information would provide context around what variables to use and potential inclusion of spatial information. 

# Appendix

Further analysis of principal components.

```{r}
for (i in 1:4) {
  for (j in (i + 1):5) {
    # Call the plot_biplot_var function for each pair
    plot <- plot_biplot_var(scores_df, loadings_df, "Class", i, j, scaling_var = 4)
    
    # Save or display the plot
    print(plot)  
    
    # can save here
    # ggsave(paste0("biplot_PC", i, "_PC", j, ".png"), plot)
  }
}
```


